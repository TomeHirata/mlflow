---
sidebar_position: 6
sidebar_label: Rewrite Prompts for New Models üÜï
---

import { APILink } from "@site/src/components/APILink";

# Rewrite Prompts for New Models (Experimental)

When migrating to a new language model, you often discover that your carefully crafted prompts don't work as well with the new model. MLflow's <APILink fn="mlflow.genai.optimize_prompts" /> API helps you **automatically rewrite prompts** to maintain output quality when switching models, using your existing application's outputs as training data.

:::tip Key Benefits

- **Seamless Model Migration**: Switch between models while preserving output quality
- **No Manual Labeling**: Use traces from your existing application as training data
- **Automatic Adaptation**: Let GEPA optimize prompts for the new model
- **Cost Optimization**: Confidently downgrade to cheaper models without quality loss
- **Provider Switching**: Migrate between providers (OpenAI ‚Üî Anthropic ‚Üî others)

:::

:::note[Version Requirements]
This feature requires **MLflow >= 3.5.0**.
:::

![](/images/adapt_prompt_flow.svg)

## When to Use This

This approach is ideal when:

- **Downgrading Models**: Moving from `gpt-4o` ‚Üí `gpt-4o-mini` to reduce costs
- **Switching Providers**: Changing from OpenAI to Anthropic or vice versa
- **Performance Optimization**: Moving to faster models while maintaining quality
- **You Have Existing Outputs**: Your current system already produces good results

## Quick Start: Model Migration Workflow

Here's a complete example of migrating from `gpt-4o` to `gpt-4o-mini` while maintaining output consistency:

### Step 1: Capture Outputs from Current Model

First, collect outputs from your existing model using MLflow tracing:

```python
import mlflow
import openai
from mlflow.genai.optimize import LLMParams
from mlflow.genai.datasets import create_dataset

# Register your current prompt
prompt = mlflow.genai.register_prompt(
    name="sentiment",
    template="""
Classify the sentiment. Answer 'positive' or 'negative' or 'neutral'.

Text: {{text}}
    """,
)


# Your current function with the source model
@mlflow.trace
def predict_fn(text: str) -> str:
    completion = openai.OpenAI().chat.completions.create(
        model="gpt-4o",  # Source model
        messages=[{"role": "user", "content": prompt.format(text=text)}],
        temperature=0,
    )
    return completion.choices[0].message.content.lower()


# Example inputs
inputs = [
    {
        "inputs": {
            "text": "This movie was absolutely fantastic! I loved every minute of it."
        }
    },
    {"inputs": {"text": "The service was terrible and the food arrived cold."}},
    {"inputs": {"text": "It was okay, nothing special but not bad either."}},
    {
        "inputs": {
            "text": "I'm so disappointed with this purchase. Complete waste of money."
        }
    },
    {"inputs": {"text": "Best experience ever! Highly recommend to everyone."}},
    {"inputs": {"text": "The product works as described. No complaints."}},
    {"inputs": {"text": "I can't believe how amazing this turned out to be!"}},
    {"inputs": {"text": "Worst customer support I've ever dealt with."}},
    {"inputs": {"text": "It's fine for the price. Gets the job done."}},
    {"inputs": {"text": "This exceeded all my expectations. Truly wonderful!"}},
]

# Collect outputs from source model
with mlflow.start_run() as run:
    for record in inputs:
        predict_fn(**record["inputs"])
```

### Step 2: Create Training Dataset from Traces

Convert the traced outputs into a training dataset:

```python
# Create dataset
dataset = create_dataset(name="sentiment_migration_dataset")

# Retrieve traces from the run
traces = mlflow.search_traces(return_type="list", run_id=run.info.run_id)

# Merge traces into dataset
dataset.merge_records(traces)
```

This automatically creates a dataset with:

- `inputs`: The input variables (`text` in this case)
- `outputs`: The actual outputs from your source model (`gpt-4o`)

### Step 3: Switch Model

Switch your LM to the target model:

```python
# Define function using target model
@mlflow.trace
def predict_fn(text: str) -> str:
    completion = openai.OpenAI().chat.completions.create(
        model="gpt-4o-mini",  # Target model
        messages=[{"role": "user", "content": prompt.format(text=text)}],
        temperature=0,
    )
    return completion.choices[0].message.content.lower()
```

You might notice the target model doesn't follow the format as consistently as the source model.

### Step 4: Optimize Prompts for Target Model

Use the collected dataset to optimize prompts for the target model:

```python
# Optimize prompts for the target model
result = mlflow.genai.optimize_prompts(
    predict_fn=predict_fn,
    train_data=dataset.to_df(),
    target_prompt_uris=[prompt.uri],
    optimizer_lm_params=LLMParams(model_name="openai:/gpt-4o-mini"),
)

# View the optimized prompt
optimized_prompt = result.optimized_prompts[0]
print(f"Optimized template: {optimized_prompt.template}")
```

The optimized prompt will include additional instructions to help `gpt-4o-mini` match the behavior of `gpt-4o`:

```
Optimized template:
Classify the sentiment of the provided text. Your response must be one of the following:
- 'positive'
- 'negative'
- 'neutral'

Ensure your response is lowercase and contains only one of these three words.

Text: {{text}}

Guidelines:
- 'positive': The text expresses satisfaction, happiness, or approval
- 'negative': The text expresses dissatisfaction, anger, or disapproval
- 'neutral': The text is factual or balanced without strong emotion

Your response must match this exact format with no additional explanation.
```

### Step 5: Use Optimized Prompt

Deploy the optimized prompt in your application:

```python
# Load the optimized prompt
optimized = mlflow.genai.load_prompt(optimized_prompt.uri)


# Use in production
@mlflow.trace
def predict_fn_optimized(text: str) -> str:
    completion = openai.OpenAI().chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": optimized.format(text=text)}],
        temperature=0,
    )
    return completion.choices[0].message.content.lower()


# Test with new inputs
test_result = predict_fn_optimized("This product is amazing!")
print(test_result)  # Output: positive
```

## Alternative: Using Existing Data

If you already have a dataset with inputs and expected outputs, you can skip the tracing step:

```python
# Prepare dataset directly
import pandas as pd

dataset = pd.DataFrame(
    {
        "inputs": [
            {"text": "This is wonderful!"},
            {"text": "Terrible experience."},
            {"text": "It's okay."},
        ],
        "outputs": [
            "positive",
            "negative",
            "neutral",
        ],
    }
)

# Optimize directly
result = mlflow.genai.optimize_prompts(
    predict_fn=predict_fn,
    train_data=dataset,
    target_prompt_uris=[prompt.uri],
    optimizer_lm_params=LLMParams(model_name="openai:/gpt-4o-mini"),
)
```

## Best Practices

### 1. Collect Sufficient Data

For best results, collect outputs from at least 20-50 diverse examples:

```python
# ‚úÖ Good: Diverse examples
inputs = [
    {"text": "Short positive."},
    {
        "text": "A much longer negative review with multiple sentences and detailed criticism."
    },
    {"text": "Neutral statement."},
    # ... more varied examples
]

# ‚ùå Poor: Too few, too similar
inputs = [
    {"text": "Good"},
    {"text": "Bad"},
]
```

### 2. Use Representative Examples

Include edge cases and challenging inputs:

```python
inputs = [
    {"text": "Absolutely fantastic!"},  # Clear positive
    {"text": "It's not bad, I guess."},  # Ambiguous
    {"text": "The food was good but service terrible."},  # Mixed sentiment
]
```

### 3. Verify Results

Always test optimized prompts using <APILink fn="mlflow.genai.evaluate" /> before production deployment.

## Troubleshooting

### Issue: Target Model Still Doesn't Match Source

**Cause**: Different models have different capabilities.

**Solution**:

- Increase training data size (aim for 50+ examples)
- Use a more powerful optimizer model
- Consider if the target model is capable of the task
- Add explicit format examples to your initial prompt

### Issue: Optimization Takes Too Long

**Solution**: Reduce dataset size or use faster optimizer:

```python
# Use subset of data
small_dataset = dataset.sample(n=20)

# Or use faster optimizer model
optimizer_lm_params = LLMParams(model_name="openai:/gpt-4o-mini")
```

### Issue: Dataset Creation Fails

**Solution**: Verify trace structure:

```python
# Check traces have required fields
traces = mlflow.search_traces(return_type="list", run_id=run.info.run_id)
for trace in traces:
    print(f"Inputs: {trace.info.request}")
    print(f"Outputs: {trace.info.response}")
```

## Measuring Success

Track how well the migration worked:

```python
# Compare source and target model on test set
from mlflow.genai import evaluate

# Evaluate optimized prompt
results = evaluate(
    data=test_dataset,
    predict_fn=predict_fn_optimized,
    scorers=[accuracy_scorer, format_scorer],
)

print(f"Accuracy: {results.metrics['accuracy']}")
print(f"Format compliance: {results.metrics['format_scorer']}")
```

## See Also

- [Optimize Prompts](/genai/prompt-registry/optimize-prompts): General prompt optimization guide
- [Create and Edit Prompts](/genai/prompt-registry/create-and-edit-prompts): Prompt Registry basics
- [Evaluate Prompts](/genai/eval-monitor/running-evaluation/prompts): Evaluate prompt performance
- [MLflow Tracing](/genai/tracing/overview): Understanding MLflow tracing
