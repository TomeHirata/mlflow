---
title: Prompt Registry
description: MLflow Prompt Registry
---

import { APILink } from "@site/src/components/APILink";
import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";
import FeatureHighlights from "@site/src/components/FeatureHighlights";
import { Package, GitBranch, Tag, LineChart, Users } from "lucide-react";

# Prompt Registry

**MLflow Prompt Registry** is a powerful tool that streamlines prompt engineering and management in your Generative AI (GenAI) applications. It enables you to version, track, and reuse prompts across your organization, helping maintain consistency and improving collaboration in prompt development.

## Key Benefits

<FeatureHighlights features={[
  {
    icon: GitBranch,
    title: "Version Control",
    description: "Track the evolution of your prompts with Git-inspired commit-based versioning and side-by-side comparison with diff highlighting. Prompt versions in MLflow are immutable, providing strong guarantees for reproducibility."
  },
  {
    icon: Tag,
    title: "Aliasing",
    description: "Build robust yet flexible deployment pipelines for prompts, allowing you to isolate prompt versions from main application code and perform tasks such as A/B testing and roll-backs with ease."
  },
  {
    icon: LineChart,
    title: "Lineage",
    description: "Seamlessly integrate with MLflow's existing features such as model tracking and evaluation for end-to-end GenAI lifecycle management."
  },
  {
    icon: Users,
    title: "Collaboration",
    description: "Share prompts across your organization with a centralized registry, enabling teams to build upon each other's work."
  },
]} />

## Getting Started

### 1. Create a Prompt

<Tabs>
  <TabItem value="ui" label="UI" default>
    <div class="flex-column">
      <div style={{ width: "70%", margin: "20px" }}>
        ![Create Prompt UI](/images/llms/prompt-registry/create-prompt-ui.png)
      </div>

      1. Run `mlflow ui` in your terminal to start the MLflow UI.
      2. Navigate to the **Prompts** tab in the MLflow UI.
      3. Click on the **Create Prompt** button.
      4. Fill in the prompt details such as name, prompt template text, and commit message (optional).
      5. Click **Create** to register the prompt.

      :::note

          Prompt template text can contain variables in `{{variable}}` format. These variables can be filled with dynamic content when using the prompt in your GenAI application. MLflow also provides the `to_single_brace_format()` API to convert templates into single brace format for frameworks like LangChain or LlamaIndex that require single brace interpolation.
      :::

    </div>

  </TabItem>
  <TabItem value="python" label="Python" default>
    <div class="flex-column">
      To create a new prompt using the Python API, use <APILink fn="mlflow.genai.register_prompt" /> API:

      ```python
      import mlflow
      from pydantic import BaseModel

      # Text prompt with response format
      text_template = """\
      Summarize content you are provided with in {{ num_sentences }} sentences.

      Sentences: {{ sentences }}
      """


      # Define response format for structured output
      class SummaryResponse(BaseModel):
          summary: str
          key_points: list[str]
          word_count: int


      # Register a text prompt
      prompt = mlflow.genai.register_prompt(
          name="summarization-prompt",
          template=text_template,
          response_format=SummaryResponse,
          # Optional: Provide a commit message to describe the changes
          commit_message="Initial commit",
          # Optional: Set tags applies to the prompt (across versions)
          tags={
              "author": "author@example.com",
              "task": "summarization",
              "language": "en",
          },
      )

      # Chat prompt example
      chat_template = [
          {"role": "system", "content": "You are a helpful {{ style }} assistant."},
          {"role": "user", "content": "{{ question }}"},
      ]

      # Register a chat prompt
      chat_prompt = mlflow.genai.register_prompt(
          name="assistant-prompt",
          template=chat_template,
          response_format={"type": "string", "description": "A helpful response"},
          commit_message="Initial chat prompt",
          tags={"type": "chat", "style": "friendly"},
      )

      # The prompt object contains information about the registered prompt
      print(f"Created prompt '{prompt.name}' (version {prompt.version})")
      ```
    </div>

  </TabItem>
</Tabs>

This creates a new prompt with the specified template text and metadata. The prompt is now available in the MLflow UI for further management.

<div style={{ width: "90%", margin: "10px" }}>
  ![Registered Prompt in UI](/images/llms/prompt-registry/registered-prompt.png)
</div>

### 2. Update the Prompt with a New Version

<Tabs>
  <TabItem value="ui" label="UI" default>
    <div class="flex-column">
      <div style={{ width: "70%", margin: "20px" }}>
        ![Update Prompt UI](/images/llms/prompt-registry/update-prompt-ui.png)
      </div>

      1. The previous step leads to the created prompt page. (If you closed the page, navigate to the **Prompts** tab in the MLflow UI and click on the prompt name.)
      2. Click on the **Create prompt Version** button.
      3. The popup dialog is pre-filled with the existing prompt text. Modify the prompt as you wish.
      4. Click **Create** to register the new version.

    </div>

  </TabItem>
  <TabItem value="python" label="Python" default>
    <div class="flex-column">
      To update an existing prompt with a new version, use the <APILink fn="mlflow.genai.register_prompt"/> API with the existing prompt name:

      ```python
      import mlflow
      from pydantic import BaseModel

      # Update text prompt with enhanced response format
      new_text_template = """\
      You are an expert summarizer. Condense the following content into exactly {{ num_sentences }} clear and informative sentences that capture the key points.

      Sentences: {{ sentences }}

      Your summary should:
      - Contain exactly {{ num_sentences }} sentences
      - Include only the most important information
      - Be written in a neutral, objective tone
      - Maintain the same level of formality as the original text
      """


      # Enhanced response format
      class EnhancedSummaryResponse(BaseModel):
          summary: str
          key_points: list[str]
          word_count: int
          confidence_score: float
          reading_level: str


      # Register a new version of an existing text prompt
      updated_prompt = mlflow.genai.register_prompt(
          name="summarization-prompt",  # Specify the existing prompt name
          template=new_text_template,
          response_format=EnhancedSummaryResponse,
          commit_message="Enhanced with confidence scoring and reading level",
          tags={
              "author": "author@example.com",
              "enhanced": "true",
          },
      )

      # Update chat prompt with additional messages
      new_chat_template = [
          {
              "role": "system",
              "content": "You are a helpful {{ style }} assistant with expertise in {{ domain }}.",
          },
          {"role": "user", "content": "{{ question }}"},
          {
              "role": "assistant",
              "content": "I'll help you with that. Let me provide a detailed response.",
          },
          {"role": "user", "content": "Please elaborate on {{ specific_aspect }}."},
      ]

      # Register a new version of an existing chat prompt
      updated_chat_prompt = mlflow.genai.register_prompt(
          name="assistant-prompt",
          template=new_chat_template,
          response_format={
              "type": "object",
              "properties": {"answer": {"type": "string"}, "sources": {"type": "array"}},
          },
          commit_message="Added multi-turn conversation support",
          tags={"type": "chat", "style": "friendly", "multi_turn": "true"},
      )
      ```
    </div>

  </TabItem>
</Tabs>

### 3. Compare the Prompt Versions

Once you have multiple versions of a prompt, you can compare them to understand the changes between versions. To compare prompt versions in the MLflow UI, click on the **Compare** tab in the prompt details page:

<div style={{ width: "90%", margin: "10px" }}>
  ![Compare Prompt
  Versions](/images/llms/prompt-registry/compare-prompt-versions.png)
</div>

### 4. Load and Use the Prompt

To use a prompt in your GenAI application, you can load it with the <APILink fn="mlflow.genai.load_prompt"/> API and fill in the variables using the <APILink fn="mlflow.entities.Prompt.format"/> method of the prompt object:

```python
import mlflow
import openai

target_text = """
MLflow is an open source platform for managing the end-to-end machine learning lifecycle.
It tackles four primary functions in the ML lifecycle: Tracking experiments, packaging ML
code for reuse, managing and deploying models, and providing a central model registry.
MLflow currently offers these functions as four components: MLflow Tracking,
MLflow Projects, MLflow Models, and MLflow Registry.
"""

# Load the prompt
prompt = mlflow.genai.load_prompt("prompts:/summarization-prompt/2")

# Check prompt properties
print(f"Is text prompt: {prompt.is_text_prompt}")
print(f"Response format: {prompt.response_format}")

# Use the prompt with an LLM
client = openai.OpenAI()
response = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": prompt.format(num_sentences=1, sentences=target_text),
        }
    ],
    model="gpt-4o-mini",
)

print(response.choices[0].message.content)
```

For chat prompts, you can load and use them similarly:

```python
import mlflow
import openai

# Load the chat prompt
chat_prompt = mlflow.genai.load_prompt("prompts:/assistant-prompt/2")

# Check prompt properties
print(f"Is text prompt: {chat_prompt.is_text_prompt}")
print(f"Response format: {chat_prompt.response_format}")

# Format the chat prompt
formatted_messages = chat_prompt.format(
    style="friendly",
    question="What is MLflow?",
)

# Use the formatted messages directly with an LLM
client = openai.OpenAI()
response = client.chat.completions.create(
    messages=formatted_messages,
    model="gpt-4o-mini",
)

print(response.choices[0].message.content)
```

### 5. Search Prompts

You can discover prompts by name, tag or other registry fields:

```python
import mlflow

# Fluent API: returns a flat list of all matching prompts
prompts = mlflow.genai.search_prompts(filter_string="task='summarization'")
print(f"Found {len(prompts)} prompts")

# For pagination control, use the client API:
from mlflow.tracking import MlflowClient

client = MlflowClient()
all_prompts = []
token = None
while True:
    page = client.search_prompts(
        filter_string="task='summarization'",
        max_results=50,
        page_token=token,
    )
    all_prompts.extend(page)
    token = page.token
    if not token:
        break
print(f"Total prompts across pages: {len(all_prompts)}")
```

## Prompt Object

The `PromptVersion` object is the core entity in MLflow Prompt Registry. It represents a versioned template that can contain variables for dynamic content. MLflow now supports two types of prompts: **text prompts** and **chat prompts**.

Key attributes of a PromptVersion object:

- `Name`: A unique identifier for the prompt.
- `Template`: The content of the prompt, which can be either:
  - A string containing text with variables in `{{variable}}` format (text prompts)
  - A list of dictionaries representing chat messages with 'role' and 'content' keys (chat prompts)
- `Version`: A sequential number representing the revision of the prompt.
- `Commit Message`: A description of the changes made in the prompt version, similar to Git commit messages.
- `Tags`: Optional key-value pairs assigned at the prompt version
  for categorization and filtering. For example, you may add tags for project name, language, etc, which apply to all versions of the prompt.
- `Alias`: An mutable named reference to the prompt. For example, you can create an alias named `production` to refer to the version used in your production system. See [Aliases](/genai/data-model/prompts#alias-management) for more details.
- `is_text_prompt`: A boolean property indicating whether the prompt is a text prompt (True) or chat prompt (False).
- `response_format`: An optional property containing the expected response structure specification, which can be used to validate or structure outputs from LLM calls.

### Prompt Types

#### Text Prompts
Text prompts use a simple string template with variables enclosed in double curly braces:

```python
text_template = "Hello {{ name }}, how are you today?"
```

#### Chat Prompts
Chat prompts use a list of message dictionaries, each with 'role' and 'content' keys:

```python
chat_template = [
    {"role": "system", "content": "You are a helpful {{ style }} assistant."},
    {"role": "user", "content": "{{ question }}"},
]
```

### Response Format
The `response_format` property allows you to specify the expected structure of responses from LLM calls. This can be either a Pydantic model class or a dictionary defining the schema:

```python
from pydantic import BaseModel


class SummaryResponse(BaseModel):
    summary: str
    key_points: list[str]
    word_count: int


# Or as a dictionary
response_format_dict = {
    "type": "object",
    "properties": {
        "summary": {"type": "string"},
        "key_points": {"type": "array", "items": {"type": "string"}},
        "word_count": {"type": "integer"},
    },
}
```

## FAQ

#### Q: How do I delete a prompt version?

A: You can delete a prompt version using the MLflow UI or Python API:

```python
import mlflow

# Delete a prompt version
client = mlflow.MlflowClient()
client.delete_prompt_version("summarization-prompt", version=2)
```

To avoid accidental deletion, you can only delete one version at a time via API. If you delete the all versions of a prompt, the prompt itself will be deleted.

#### Q: Can I update the prompt template of an existing prompt version?

A: No, prompt versions are immutable once created. To update a prompt, create a new version with the desired changes.

#### Q: Can I use prompt templates with frameworks like LangChain or LlamaIndex?

A: Yes, you can load prompts from MLflow and use them with any framework. For example, the following example demonstrates how to use a prompt registered in MLflow with LangChain. Also refer to [Logging Prompts with LangChain](/genai/prompt-version-mgmt/prompt-registry/log-with-model#example-1-logging-prompts-with-langchain) for more details.

```python
import mlflow
from langchain.prompts import PromptTemplate

# Load prompt from MLflow
prompt = mlflow.genai.load_prompt("question_answering")

# Convert the prompt to single brace format for LangChain (MLflow uses double braces),
# using the `to_single_brace_format` method.
langchain_prompt = PromptTemplate.from_template(prompt.to_single_brace_format())
print(langchain_prompt.input_variables)
# Output: ['num_sentences', 'sentences']
```

#### Q: Is Prompt Registry integrated with the Prompt Engineering UI?

A. Direct integration between the Prompt Registry and the Prompt Engineering UI is coming soon. In the meantime, you can iterate on prompt template in the Prompt Engineering UI and register the final version in the Prompt Registry by manually copying the prompt template.
